{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Multimodal Capabilities with Gemini 3 Pro\n",
    "\n",
    "> **Created by [Build Fast with AI](https://www.buildfastwithai.com)**\n",
    "\n",
    "This notebook explores Gemini 3 Pro's powerful multimodal capabilities, including processing text, images, video, and audio together.\n",
    "\n",
    "## What you'll learn:\n",
    "- Understanding multimodal AI\n",
    "- Working with text and images together\n",
    "- Video understanding and analysis\n",
    "- Audio processing capabilities\n",
    "- Combining multiple modalities\n",
    "- Building multimodal applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-generativeai pillow opencv-python moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import io\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure API key\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "except:\n",
    "    GOOGLE_API_KEY = os.environ.get('GOOGLE_API_KEY', 'your-api-key-here')\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Text + Image Understanding\n",
    "\n",
    "Gemini can process text and images together, understanding relationships between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample image with text and objects\n",
    "def create_sample_scene():\n",
    "    \"\"\"Create an image of a simple scene.\"\"\"\n",
    "    img = Image.new('RGB', (600, 400), color='lightblue')\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    # Draw sun\n",
    "    draw.ellipse([500, 50, 550, 100], fill='yellow', outline='orange')\n",
    "    \n",
    "    # Draw ground\n",
    "    draw.rectangle([0, 300, 600, 400], fill='green')\n",
    "    \n",
    "    # Draw house\n",
    "    draw.rectangle([150, 200, 300, 300], fill='brown', outline='black')\n",
    "    draw.polygon([150, 200, 225, 150, 300, 200], fill='red')  # Roof\n",
    "    draw.rectangle([200, 240, 250, 300], fill='lightblue')  # Door\n",
    "    \n",
    "    # Draw tree\n",
    "    draw.rectangle([400, 250, 420, 300], fill='brown')  # Trunk\n",
    "    draw.ellipse([380, 200, 440, 260], fill='darkgreen')  # Leaves\n",
    "    \n",
    "    # Add text\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\", 24)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    draw.text((200, 20), \"My House\", fill='black', font=font)\n",
    "    \n",
    "    return img\n",
    "\n",
    "scene_image = create_sample_scene()\n",
    "scene_image.save('scene.png')\n",
    "display(scene_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the scene with multimodal prompting\n",
    "model = genai.GenerativeModel('gemini-3-pro')\n",
    "\n",
    "questions = [\n",
    "    \"Describe everything you see in this image.\",\n",
    "    \"What text is written in the image?\",\n",
    "    \"What time of day does this scene represent and why?\",\n",
    "    \"Count the objects in the scene.\",\n",
    "    \"If this were a real place, what would you hear?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    response = model.generate_content([question, scene_image])\n",
    "    print(f\"A: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Multiple Images Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple related images\n",
    "def create_sequence_images():\n",
    "    \"\"\"Create a sequence of images showing progression.\"\"\"\n",
    "    images = []\n",
    "    \n",
    "    for i, (color, label) in enumerate([('red', 'Morning'), ('yellow', 'Noon'), ('orange', 'Evening')]):\n",
    "        img = Image.new('RGB', (300, 200), color=color)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        \n",
    "        try:\n",
    "            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\", 30)\n",
    "        except:\n",
    "            font = ImageFont.load_default()\n",
    "        \n",
    "        draw.text((80, 85), label, fill='white', font=font)\n",
    "        images.append(img)\n",
    "        img.save(f'time_{i}.png')\n",
    "    \n",
    "    return images\n",
    "\n",
    "sequence_images = create_sequence_images()\n",
    "\n",
    "print(\"Created sequence:\")\n",
    "for img in sequence_images:\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze multiple images together\n",
    "print(\"Analyzing image sequence...\\n\")\n",
    "\n",
    "response = model.generate_content([\n",
    "    \"Analyze these three images as a sequence. What story do they tell? What changes between them?\",\n",
    "    sequence_images[0],\n",
    "    sequence_images[1],\n",
    "    sequence_images[2]\n",
    "])\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Visual Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualQA:\n",
    "    \"\"\"Visual Question Answering system.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = genai.GenerativeModel('gemini-3-pro')\n",
    "        self.context_image = None\n",
    "        self.history = []\n",
    "    \n",
    "    def set_image(self, image: Image.Image):\n",
    "        \"\"\"Set the context image for Q&A.\"\"\"\n",
    "        self.context_image = image\n",
    "        self.history = []\n",
    "    \n",
    "    def ask(self, question: str) -> str:\n",
    "        \"\"\"Ask a question about the image.\"\"\"\n",
    "        if self.context_image is None:\n",
    "            return \"Please set an image first using set_image()\"\n",
    "        \n",
    "        # Include previous Q&A in context\n",
    "        context = \"\\n\".join([\n",
    "            f\"Q: {item['question']}\\nA: {item['answer']}\"\n",
    "            for item in self.history[-3:]  # Last 3 Q&A pairs\n",
    "        ])\n",
    "        \n",
    "        full_prompt = f\"\"\"\n",
    "        Previous conversation:\n",
    "        {context}\n",
    "        \n",
    "        New question: {question}\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.model.generate_content([full_prompt, self.context_image])\n",
    "        answer = response.text\n",
    "        \n",
    "        self.history.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "        \n",
    "        return answer\n",
    "\n",
    "# Test Visual QA\n",
    "vqa = VisualQA()\n",
    "vqa.set_image(scene_image)\n",
    "\n",
    "questions = [\n",
    "    \"What color is the house?\",\n",
    "    \"Is there a tree in the image?\",\n",
    "    \"Where is the tree located relative to the house?\",\n",
    "    \"What's written at the top of the image?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    answer = vqa.ask(q)\n",
    "    print(f\"A: {answer}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. Document Understanding\n",
    "\n",
    "Gemini can understand documents with mixed content (text, images, tables, diagrams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mock_document():\n",
    "    \"\"\"Create a mock document with text and simple table.\"\"\"\n",
    "    img = Image.new('RGB', (800, 600), color='white')\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    try:\n",
    "        title_font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\", 32)\n",
    "        text_font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 16)\n",
    "    except:\n",
    "        title_font = ImageFont.load_default()\n",
    "        text_font = ImageFont.load_default()\n",
    "    \n",
    "    # Title\n",
    "    draw.text((50, 30), \"Quarterly Sales Report\", fill='black', font=title_font)\n",
    "    \n",
    "    # Text content\n",
    "    text_content = [\n",
    "        \"Q3 2024 Performance Summary\",\n",
    "        \"\",\n",
    "        \"Total Revenue: $1.2M\",\n",
    "        \"Growth: +15% YoY\",\n",
    "        \"\",\n",
    "        \"Regional Breakdown:\"\n",
    "    ]\n",
    "    \n",
    "    y = 100\n",
    "    for line in text_content:\n",
    "        draw.text((50, y), line, fill='black', font=text_font)\n",
    "        y += 30\n",
    "    \n",
    "    # Simple table\n",
    "    table_y = y + 20\n",
    "    draw.rectangle([50, table_y, 500, table_y + 150], outline='black', width=2)\n",
    "    \n",
    "    # Table headers\n",
    "    draw.text((60, table_y + 10), \"Region\", fill='black', font=text_font)\n",
    "    draw.text((200, table_y + 10), \"Revenue\", fill='black', font=text_font)\n",
    "    draw.text((350, table_y + 10), \"Growth\", fill='black', font=text_font)\n",
    "    \n",
    "    # Table rows\n",
    "    rows = [\n",
    "        (\"North\", \"$400K\", \"+12%\"),\n",
    "        (\"South\", \"$350K\", \"+18%\"),\n",
    "        (\"East\", \"$300K\", \"+10%\"),\n",
    "        (\"West\", \"$150K\", \"+25%\")\n",
    "    ]\n",
    "    \n",
    "    row_y = table_y + 40\n",
    "    for region, revenue, growth in rows:\n",
    "        draw.text((60, row_y), region, fill='black', font=text_font)\n",
    "        draw.text((200, row_y), revenue, fill='black', font=text_font)\n",
    "        draw.text((350, row_y), growth, fill='black', font=text_font)\n",
    "        row_y += 30\n",
    "    \n",
    "    return img\n",
    "\n",
    "doc_image = create_mock_document()\n",
    "doc_image.save('mock_document.png')\n",
    "display(doc_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Q&A\n",
    "doc_questions = [\n",
    "    \"What is the title of this document?\",\n",
    "    \"What was the total revenue?\",\n",
    "    \"Which region had the highest growth?\",\n",
    "    \"Which region had the lowest revenue?\",\n",
    "    \"What is the year-over-year growth rate?\"\n",
    "]\n",
    "\n",
    "print(\"Document Q&A:\\n\")\n",
    "for q in doc_questions:\n",
    "    print(f\"Q: {q}\")\n",
    "    response = model.generate_content([q, doc_image])\n",
    "    print(f\"A: {response.text}\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Chart and Graph Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample chart\n",
    "def create_sample_chart():\n",
    "    \"\"\"Create a bar chart.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    categories = ['Product A', 'Product B', 'Product C', 'Product D']\n",
    "    values = [450, 380, 520, 290]\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
    "    \n",
    "    bars = ax.bar(categories, values, color=colors)\n",
    "    \n",
    "    ax.set_ylabel('Sales (units)', fontsize=12)\n",
    "    ax.set_title('Product Sales Comparison - Q3 2024', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylim(0, 600)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height)}',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sales_chart.png', dpi=100, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return Image.open('sales_chart.png')\n",
    "\n",
    "chart_image = create_sample_chart()\n",
    "display(chart_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the chart\n",
    "chart_questions = [\n",
    "    \"What type of chart is this?\",\n",
    "    \"What does this chart show?\",\n",
    "    \"Which product has the highest sales?\",\n",
    "    \"What's the difference between the best and worst performing products?\",\n",
    "    \"What insights can you derive from this data?\",\n",
    "    \"Suggest improvements for the underperforming products.\"\n",
    "]\n",
    "\n",
    "print(\"Chart Analysis:\\n\")\n",
    "for q in chart_questions:\n",
    "    print(f\"Q: {q}\")\n",
    "    response = model.generate_content([q, chart_image])\n",
    "    print(f\"A: {response.text}\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 7. Video Understanding\n",
    "\n",
    "Gemini can analyze video content and answer questions about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This is a conceptual example. In production, you would upload actual video files.\n",
    "\n",
    "class VideoAnalyzer:\n",
    "    \"\"\"Analyze video content.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = genai.GenerativeModel('gemini-3-pro')\n",
    "    \n",
    "    def analyze_video(self, video_path: str) -> dict:\n",
    "        \"\"\"Analyze a video file.\"\"\"\n",
    "        # Upload video file\n",
    "        video_file = genai.upload_file(path=video_path)\n",
    "        \n",
    "        analyses = {}\n",
    "        \n",
    "        # Overall description\n",
    "        response = self.model.generate_content([\n",
    "            \"Describe what happens in this video.\",\n",
    "            video_file\n",
    "        ])\n",
    "        analyses['description'] = response.text\n",
    "        \n",
    "        # Key events\n",
    "        response = self.model.generate_content([\n",
    "            \"List the key events or scenes in this video in chronological order.\",\n",
    "            video_file\n",
    "        ])\n",
    "        analyses['key_events'] = response.text\n",
    "        \n",
    "        # Objects/people\n",
    "        response = self.model.generate_content([\n",
    "            \"What objects, people, or elements appear in this video?\",\n",
    "            video_file\n",
    "        ])\n",
    "        analyses['elements'] = response.text\n",
    "        \n",
    "        return analyses\n",
    "    \n",
    "    def answer_video_question(self, video_path: str, question: str) -> str:\n",
    "        \"\"\"Answer a specific question about the video.\"\"\"\n",
    "        video_file = genai.upload_file(path=video_path)\n",
    "        response = self.model.generate_content([question, video_file])\n",
    "        return response.text\n",
    "\n",
    "print(\"Video analyzer created.\")\n",
    "print(\"\\nTo use:\")\n",
    "print(\"1. Upload a video file to Gemini\")\n",
    "print(\"2. Use analyze_video() or answer_video_question()\")\n",
    "print(\"\\nSupported formats: MP4, MOV, AVI, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 8. Audio Understanding\n",
    "\n",
    "Gemini can process and understand audio content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioAnalyzer:\n",
    "    \"\"\"Analyze audio content.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = genai.GenerativeModel('gemini-3-pro')\n",
    "    \n",
    "    def transcribe(self, audio_path: str) -> str:\n",
    "        \"\"\"Transcribe audio to text.\"\"\"\n",
    "        audio_file = genai.upload_file(path=audio_path)\n",
    "        response = self.model.generate_content([\n",
    "            \"Transcribe this audio file.\",\n",
    "            audio_file\n",
    "        ])\n",
    "        return response.text\n",
    "    \n",
    "    def summarize_audio(self, audio_path: str) -> str:\n",
    "        \"\"\"Summarize audio content.\"\"\"\n",
    "        audio_file = genai.upload_file(path=audio_path)\n",
    "        response = self.model.generate_content([\n",
    "            \"Summarize the main points discussed in this audio.\",\n",
    "            audio_file\n",
    "        ])\n",
    "        return response.text\n",
    "    \n",
    "    def extract_insights(self, audio_path: str) -> dict:\n",
    "        \"\"\"Extract key insights from audio.\"\"\"\n",
    "        audio_file = genai.upload_file(path=audio_path)\n",
    "        \n",
    "        insights = {}\n",
    "        \n",
    "        # Topics\n",
    "        response = self.model.generate_content([\n",
    "            \"What topics are discussed in this audio?\",\n",
    "            audio_file\n",
    "        ])\n",
    "        insights['topics'] = response.text\n",
    "        \n",
    "        # Sentiment\n",
    "        response = self.model.generate_content([\n",
    "            \"What is the overall tone and sentiment of this audio?\",\n",
    "            audio_file\n",
    "        ])\n",
    "        insights['sentiment'] = response.text\n",
    "        \n",
    "        # Action items\n",
    "        response = self.model.generate_content([\n",
    "            \"Extract any action items or next steps mentioned in this audio.\",\n",
    "            audio_file\n",
    "        ])\n",
    "        insights['action_items'] = response.text\n",
    "        \n",
    "        return insights\n",
    "\n",
    "print(\"Audio analyzer created.\")\n",
    "print(\"\\nCapabilities:\")\n",
    "print(\"- Transcription\")\n",
    "print(\"- Summarization\")\n",
    "print(\"- Topic extraction\")\n",
    "print(\"- Sentiment analysis\")\n",
    "print(\"- Action item extraction\")\n",
    "print(\"\\nSupported formats: MP3, WAV, M4A, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 9. Multimodal RAG (Retrieval-Augmented Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalRAG:\n",
    "    \"\"\"RAG system that works with multiple modalities.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = genai.GenerativeModel('gemini-3-pro')\n",
    "        self.knowledge_base = []\n",
    "    \n",
    "    def add_text(self, text: str, metadata: dict = None):\n",
    "        \"\"\"Add text to knowledge base.\"\"\"\n",
    "        self.knowledge_base.append({\n",
    "            'type': 'text',\n",
    "            'content': text,\n",
    "            'metadata': metadata or {}\n",
    "        })\n",
    "    \n",
    "    def add_image(self, image: Image.Image, description: str = None, metadata: dict = None):\n",
    "        \"\"\"Add image to knowledge base.\"\"\"\n",
    "        # Generate description if not provided\n",
    "        if description is None:\n",
    "            response = self.model.generate_content([\"Describe this image in detail.\", image])\n",
    "            description = response.text\n",
    "        \n",
    "        self.knowledge_base.append({\n",
    "            'type': 'image',\n",
    "            'content': image,\n",
    "            'description': description,\n",
    "            'metadata': metadata or {}\n",
    "        })\n",
    "    \n",
    "    def query(self, question: str, include_images: bool = True) -> dict:\n",
    "        \"\"\"Query the multimodal knowledge base.\"\"\"\n",
    "        # Gather relevant context\n",
    "        text_context = [item['content'] for item in self.knowledge_base if item['type'] == 'text']\n",
    "        image_context = [item for item in self.knowledge_base if item['type'] == 'image']\n",
    "        \n",
    "        # Build prompt\n",
    "        prompt_parts = [\n",
    "            f\"Answer this question using the provided context:\\n\\nQuestion: {question}\\n\\n\"\n",
    "        ]\n",
    "        \n",
    "        # Add text context\n",
    "        if text_context:\n",
    "            prompt_parts.append(\"Text Context:\\n\" + \"\\n\\n\".join(text_context))\n",
    "        \n",
    "        # Add images\n",
    "        if include_images and image_context:\n",
    "            prompt_parts.append(\"\\nRelevant images are also provided.\")\n",
    "            for item in image_context:\n",
    "                prompt_parts.append(item['content'])\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.model.generate_content(prompt_parts)\n",
    "        \n",
    "        return {\n",
    "            'answer': response.text,\n",
    "            'sources': {\n",
    "                'text_items': len(text_context),\n",
    "                'image_items': len(image_context)\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Test Multimodal RAG\n",
    "rag = MultimodalRAG()\n",
    "\n",
    "# Add content\n",
    "rag.add_text(\"\"\"\n",
    "Product Description:\n",
    "The SmartHome Hub 3000 is our latest home automation controller.\n",
    "Features include voice control, mobile app, and compatibility with 100+ smart devices.\n",
    "Available in white and black. Price: $149.99\n",
    "\"\"\")\n",
    "\n",
    "rag.add_text(\"\"\"\n",
    "Customer Reviews:\n",
    "- \"Easy to set up and use!\" - 5 stars\n",
    "- \"Great value for money\" - 4 stars\n",
    "- \"Voice control is very responsive\" - 5 stars\n",
    "\"\"\")\n",
    "\n",
    "# Add images\n",
    "rag.add_image(scene_image, description=\"Product showcase image\")\n",
    "\n",
    "# Query the system\n",
    "questions = [\n",
    "    \"What is the SmartHome Hub 3000?\",\n",
    "    \"How much does it cost?\",\n",
    "    \"What do customers say about it?\",\n",
    "    \"What colors is it available in?\"\n",
    "]\n",
    "\n",
    "print(\"Multimodal RAG System Test:\\n\")\n",
    "for q in questions:\n",
    "    print(f\"Q: {q}\")\n",
    "    result = rag.query(q)\n",
    "    print(f\"A: {result['answer']}\")\n",
    "    print(f\"Sources: {result['sources']}\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 10. Advanced Multimodal Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalAssistant:\n",
    "    \"\"\"A comprehensive multimodal AI assistant.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = genai.GenerativeModel('gemini-3-pro')\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def process_input(self, text: str = None, image: Image.Image = None, \n",
    "                     video_path: str = None, audio_path: str = None) -> str:\n",
    "        \"\"\"Process any combination of inputs.\"\"\"\n",
    "        inputs = []\n",
    "        \n",
    "        if text:\n",
    "            inputs.append(text)\n",
    "        \n",
    "        if image:\n",
    "            inputs.append(image)\n",
    "        \n",
    "        if video_path:\n",
    "            video_file = genai.upload_file(path=video_path)\n",
    "            inputs.append(video_file)\n",
    "        \n",
    "        if audio_path:\n",
    "            audio_file = genai.upload_file(path=audio_path)\n",
    "            inputs.append(audio_file)\n",
    "        \n",
    "        if not inputs:\n",
    "            return \"No input provided\"\n",
    "        \n",
    "        # Add conversation history\n",
    "        if self.conversation_history:\n",
    "            context = \"Previous conversation:\\n\" + \"\\n\".join([\n",
    "                f\"User: {item['user']}\\nAssistant: {item['assistant']}\"\n",
    "                for item in self.conversation_history[-3:]\n",
    "            ])\n",
    "            inputs.insert(0, context)\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.model.generate_content(inputs)\n",
    "        \n",
    "        # Save to history\n",
    "        self.conversation_history.append({\n",
    "            'user': text or \"[multimodal input]\",\n",
    "            'assistant': response.text\n",
    "        })\n",
    "        \n",
    "        return response.text\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history.\"\"\"\n",
    "        self.conversation_history = []\n",
    "\n",
    "# Test the assistant\n",
    "assistant = MultimodalAssistant()\n",
    "\n",
    "print(\"Test 1: Text + Image\")\n",
    "response = assistant.process_input(\n",
    "    text=\"What's in this image and what time of day does it represent?\",\n",
    "    image=scene_image\n",
    ")\n",
    "print(f\"Response: {response}\\n\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Test 2: Follow-up question (uses conversation history)\")\n",
    "response = assistant.process_input(\n",
    "    text=\"What colors are prominent in that image?\"\n",
    ")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## 11. Best Practices for Multimodal AI\n",
    "\n",
    "### Input Preparation:\n",
    "\n",
    "1. **Image Quality**: Use clear, well-lit images\n",
    "2. **Resolution**: Balance quality and file size\n",
    "3. **File Formats**: Use supported formats (JPEG, PNG, WebP)\n",
    "4. **Video Length**: Keep videos concise for better analysis\n",
    "5. **Audio Quality**: Ensure clear audio without background noise\n",
    "\n",
    "### Prompt Engineering:\n",
    "\n",
    "1. **Be Specific**: Clearly describe what you want to know\n",
    "2. **Context**: Provide relevant context for better understanding\n",
    "3. **Sequential**: For multiple items, indicate order if important\n",
    "4. **Comparative**: Use comparison prompts for multiple inputs\n",
    "\n",
    "### Performance Optimization:\n",
    "\n",
    "1. **Batch Processing**: Group similar analyses\n",
    "2. **Caching**: Cache results for repeated queries\n",
    "3. **Preprocessing**: Optimize media before sending\n",
    "4. **Selective Upload**: Only upload necessary content\n",
    "\n",
    "### Application Design:\n",
    "\n",
    "1. **Progressive Enhancement**: Start simple, add complexity\n",
    "2. **Error Handling**: Handle unsupported formats gracefully\n",
    "3. **User Feedback**: Show processing status\n",
    "4. **Result Presentation**: Display results clearly\n",
    "\n",
    "### Security & Privacy:\n",
    "\n",
    "1. **Data Handling**: Handle sensitive content appropriately\n",
    "2. **User Consent**: Get consent before processing personal media\n",
    "3. **Temporary Storage**: Delete uploaded content after processing\n",
    "4. **Access Control**: Implement proper authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## 12. Real-World Applications\n",
    "\n",
    "### Multimodal AI Use Cases:\n",
    "\n",
    "1. **Education**:\n",
    "   - Analyze diagrams and explain concepts\n",
    "   - Grade assignments with mixed content\n",
    "   - Create interactive learning experiences\n",
    "\n",
    "2. **Healthcare**:\n",
    "   - Medical image analysis with reports\n",
    "   - Patient history with imaging\n",
    "   - Research document analysis\n",
    "\n",
    "3. **Business**:\n",
    "   - Document processing (invoices, reports)\n",
    "   - Product catalog management\n",
    "   - Meeting transcription and analysis\n",
    "\n",
    "4. **Content Creation**:\n",
    "   - Video content analysis\n",
    "   - Social media management\n",
    "   - Automated captioning\n",
    "\n",
    "5. **E-commerce**:\n",
    "   - Visual search\n",
    "   - Product recommendations\n",
    "   - Quality control\n",
    "\n",
    "6. **Accessibility**:\n",
    "   - Image descriptions for visually impaired\n",
    "   - Audio transcription for hearing impaired\n",
    "   - Document reader for learning disabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Explore advanced multimodal applications:\n",
    "- Build a multimodal search engine\n",
    "- Create content moderation systems\n",
    "- Develop interactive educational tools\n",
    "- Build accessibility applications\n",
    "- Create automated content generation pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## Learn More\n",
    "\n",
    "Master multimodal AI and build cutting-edge applications with the **[Gen AI Crash Course](https://www.buildfastwithai.com/genai-course)** by Build Fast with AI!\n",
    "\n",
    "**Created by [Build Fast with AI](https://www.buildfastwithai.com)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
