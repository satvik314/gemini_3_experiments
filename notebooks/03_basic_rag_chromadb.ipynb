{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RAG with ChromaDB and Gemini 3 Pro\n",
    "\n",
    "> **Created by [Build Fast with AI](https://www.buildfastwithai.com)**\n",
    "\n",
    "This notebook demonstrates how to build a Retrieval Augmented Generation (RAG) system using ChromaDB and Gemini 3 Pro.\n",
    "\n",
    "## What you'll learn:\n",
    "- What is RAG and why it's important\n",
    "- Setting up ChromaDB for vector storage\n",
    "- Creating embeddings with Google's embedding models\n",
    "- Building a simple document Q&A system\n",
    "- Best practices for RAG implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-generativeai chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import os\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure API key\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "except:\n",
    "    GOOGLE_API_KEY = os.environ.get('GOOGLE_API_KEY', 'your-api-key-here')\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding RAG\n",
    "\n",
    "**Retrieval Augmented Generation (RAG)** combines:\n",
    "- **Retrieval**: Finding relevant information from a knowledge base\n",
    "- **Generation**: Using an LLM to generate answers based on retrieved context\n",
    "\n",
    "Benefits:\n",
    "- Reduces hallucinations\n",
    "- Provides source attribution\n",
    "- Enables domain-specific knowledge\n",
    "- No need to fine-tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sample Documents\n",
    "\n",
    "Let's create a knowledge base about AI and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    {\n",
    "        \"id\": \"doc1\",\n",
    "        \"text\": \"Machine Learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing computer programs that can access data and use it to learn for themselves.\",\n",
    "        \"metadata\": {\"category\": \"basics\", \"topic\": \"machine learning\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc2\",\n",
    "        \"text\": \"Deep Learning is a subset of machine learning that uses neural networks with multiple layers. These deep neural networks are capable of learning and making intelligent decisions on their own. Deep learning has been particularly successful in image recognition, natural language processing, and speech recognition.\",\n",
    "        \"metadata\": {\"category\": \"advanced\", \"topic\": \"deep learning\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc3\",\n",
    "        \"text\": \"Natural Language Processing (NLP) is a branch of AI that helps computers understand, interpret, and manipulate human language. NLP draws from many disciplines, including computer science and computational linguistics, to bridge the gap between human communication and computer understanding.\",\n",
    "        \"metadata\": {\"category\": \"basics\", \"topic\": \"nlp\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc4\",\n",
    "        \"text\": \"Transformers are a type of neural network architecture that have revolutionized NLP. Introduced in 2017, they use self-attention mechanisms to process sequential data. Models like BERT, GPT, and T5 are all based on the transformer architecture.\",\n",
    "        \"metadata\": {\"category\": \"advanced\", \"topic\": \"transformers\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc5\",\n",
    "        \"text\": \"Supervised Learning is a type of machine learning where the model is trained on labeled data. The algorithm learns from the training dataset and makes predictions on unseen data. Common applications include classification and regression tasks.\",\n",
    "        \"metadata\": {\"category\": \"basics\", \"topic\": \"supervised learning\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc6\",\n",
    "        \"text\": \"Unsupervised Learning involves training a model on unlabeled data. The system tries to learn patterns and structure from the data without explicit guidance. Clustering and dimensionality reduction are common unsupervised learning techniques.\",\n",
    "        \"metadata\": {\"category\": \"basics\", \"topic\": \"unsupervised learning\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc7\",\n",
    "        \"text\": \"Reinforcement Learning is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize cumulative reward. It has been successfully applied to game playing, robotics, and autonomous systems.\",\n",
    "        \"metadata\": {\"category\": \"advanced\", \"topic\": \"reinforcement learning\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc8\",\n",
    "        \"text\": \"Large Language Models (LLMs) are AI models trained on vast amounts of text data. They can generate human-like text, answer questions, write code, and perform various language tasks. Examples include GPT-4, PaLM, and Gemini.\",\n",
    "        \"metadata\": {\"category\": \"advanced\", \"topic\": \"llm\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Created knowledge base with {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting Up ChromaDB\n",
    "\n",
    "ChromaDB is a vector database optimized for embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB client\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Create a collection\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"ai_knowledge_base\",\n",
    "    metadata={\"description\": \"AI and ML concepts\"}\n",
    ")\n",
    "\n",
    "print(f\"Collection created: {collection.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating Embeddings with Google's Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    \"\"\"Get embedding for text using Google's embedding model.\"\"\"\n",
    "    result = genai.embed_content(\n",
    "        model=\"models/embedding-001\",\n",
    "        content=text,\n",
    "        task_type=\"retrieval_document\"\n",
    "    )\n",
    "    return result['embedding']\n",
    "\n",
    "# Test the embedding function\n",
    "test_embedding = get_embedding(\"This is a test sentence.\")\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")\n",
    "print(f\"First 5 values: {test_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adding Documents to ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for ChromaDB\n",
    "ids = [doc[\"id\"] for doc in documents]\n",
    "texts = [doc[\"text\"] for doc in documents]\n",
    "metadatas = [doc[\"metadata\"] for doc in documents]\n",
    "\n",
    "# Generate embeddings for all documents\n",
    "print(\"Generating embeddings...\")\n",
    "embeddings = [get_embedding(text) for text in texts]\n",
    "print(f\"Generated {len(embeddings)} embeddings\")\n",
    "\n",
    "# Add to collection\n",
    "collection.add(\n",
    "    ids=ids,\n",
    "    embeddings=embeddings,\n",
    "    documents=texts,\n",
    "    metadatas=metadatas\n",
    ")\n",
    "\n",
    "print(f\"\\nAdded {collection.count()} documents to the collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Semantic Search\n",
    "\n",
    "Now we can search for relevant documents using natural language queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_documents(query, n_results=3):\n",
    "    \"\"\"Search for relevant documents using semantic search.\"\"\"\n",
    "    # Get embedding for query\n",
    "    query_embedding_result = genai.embed_content(\n",
    "        model=\"models/embedding-001\",\n",
    "        content=query,\n",
    "        task_type=\"retrieval_query\"\n",
    "    )\n",
    "    query_embedding = query_embedding_result['embedding']\n",
    "    \n",
    "    # Search in ChromaDB\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test search\n",
    "query = \"What are neural networks?\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "results = search_documents(query)\n",
    "\n",
    "print(\"Top 3 relevant documents:\\n\")\n",
    "for i, (doc, metadata, distance) in enumerate(zip(\n",
    "    results['documents'][0],\n",
    "    results['metadatas'][0],\n",
    "    results['distances'][0]\n",
    "), 1):\n",
    "    print(f\"{i}. [Score: {1-distance:.3f}] {metadata['topic']}\")\n",
    "    print(f\"   {doc[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Building the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(question, n_results=3):\n",
    "    \"\"\"\n",
    "    Answer a question using RAG:\n",
    "    1. Retrieve relevant documents\n",
    "    2. Generate answer using Gemini with context\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    results = search_documents(question, n_results=n_results)\n",
    "    \n",
    "    # Step 2: Prepare context from retrieved documents\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Document {i+1}: {doc}\"\n",
    "        for i, doc in enumerate(results['documents'][0])\n",
    "    ])\n",
    "    \n",
    "    # Step 3: Create prompt with context\n",
    "    prompt = f\"\"\"\n",
    "Based on the following context, answer the question. If the answer cannot be found in the context, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    \n",
    "    # Step 4: Generate answer using Gemini\n",
    "    model = genai.GenerativeModel('gemini-3-pro')\n",
    "    response = model.generate_content(prompt)\n",
    "    \n",
    "    return {\n",
    "        \"answer\": response.text,\n",
    "        \"sources\": results['metadatas'][0],\n",
    "        \"source_texts\": results['documents'][0]\n",
    "    }\n",
    "\n",
    "# Test RAG system\n",
    "question = \"What is the difference between supervised and unsupervised learning?\"\n",
    "print(f\"Question: {question}\\n\")\n",
    "\n",
    "result = rag_query(question)\n",
    "\n",
    "print(\"Answer:\")\n",
    "display(Markdown(result['answer']))\n",
    "\n",
    "print(\"\\n\\nSources used:\")\n",
    "for i, source in enumerate(result['sources'], 1):\n",
    "    print(f\"{i}. Topic: {source['topic']} (Category: {source['category']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive RAG Q&A System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSystem:\n",
    "    def __init__(self, collection):\n",
    "        self.collection = collection\n",
    "        self.model = genai.GenerativeModel('gemini-3-pro')\n",
    "    \n",
    "    def ask(self, question, n_results=3, show_sources=True):\n",
    "        \"\"\"Ask a question and get an answer with sources.\"\"\"\n",
    "        result = rag_query(question, n_results=n_results)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        display(Markdown(result['answer']))\n",
    "        \n",
    "        if show_sources:\n",
    "            print(\"\\n\\nSources:\")\n",
    "            for i, (source, text) in enumerate(zip(\n",
    "                result['sources'],\n",
    "                result['source_texts']\n",
    "            ), 1):\n",
    "                print(f\"\\n{i}. [{source['category'].upper()}] {source['topic']}\")\n",
    "                print(f\"   {text[:150]}...\")\n",
    "\n",
    "# Create RAG system\n",
    "rag_system = RAGSystem(collection)\n",
    "\n",
    "# Test with multiple questions\n",
    "questions = [\n",
    "    \"What are transformers in AI?\",\n",
    "    \"Explain reinforcement learning\",\n",
    "    \"What is NLP used for?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    rag_system.ask(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced: Filtering and Metadata Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query_with_filter(question, category_filter=None, n_results=3):\n",
    "    \"\"\"RAG query with metadata filtering.\"\"\"\n",
    "    # Get query embedding\n",
    "    query_embedding_result = genai.embed_content(\n",
    "        model=\"models/embedding-001\",\n",
    "        content=question,\n",
    "        task_type=\"retrieval_query\"\n",
    "    )\n",
    "    query_embedding = query_embedding_result['embedding']\n",
    "    \n",
    "    # Build where clause for filtering\n",
    "    where_clause = {\"category\": category_filter} if category_filter else None\n",
    "    \n",
    "    # Search with filter\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results,\n",
    "        where=where_clause\n",
    "    )\n",
    "    \n",
    "    # Generate answer\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Document {i+1}: {doc}\"\n",
    "        for i, doc in enumerate(results['documents'][0])\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Based on the following context, answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    \n",
    "    model = genai.GenerativeModel('gemini-3-pro')\n",
    "    response = model.generate_content(prompt)\n",
    "    \n",
    "    return response.text, results['metadatas'][0]\n",
    "\n",
    "# Test with filtering\n",
    "print(\"Searching only in 'basics' category:\\n\")\n",
    "answer, sources = rag_query_with_filter(\n",
    "    \"Explain machine learning\",\n",
    "    category_filter=\"basics\"\n",
    ")\n",
    "\n",
    "display(Markdown(answer))\n",
    "print(\"\\nSources:\", [s['topic'] for s in sources])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Best Practices for RAG\n",
    "\n",
    "### Key Considerations:\n",
    "\n",
    "1. **Chunk Size**: Split documents into optimal sizes (typically 200-500 tokens)\n",
    "2. **Embedding Quality**: Use domain-specific embeddings when possible\n",
    "3. **Retrieval Strategy**: Experiment with different retrieval methods\n",
    "4. **Context Window**: Balance between context length and relevance\n",
    "5. **Source Attribution**: Always provide sources for transparency\n",
    "6. **Evaluation**: Regularly evaluate answer quality and relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Document chunking\n",
    "def chunk_text(text, chunk_size=200, overlap=50):\n",
    "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "        if i + chunk_size >= len(words):\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test chunking\n",
    "long_text = \" \".join([\"This is a test sentence.\"] * 100)\n",
    "chunks = chunk_text(long_text, chunk_size=50, overlap=10)\n",
    "print(f\"Created {len(chunks)} chunks from text with {len(long_text.split())} words\")\n",
    "print(f\"First chunk: {chunks[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Explore advanced RAG with LangChain\n",
    "- Build agentic RAG systems\n",
    "- Create production-ready RAG applications\n",
    "\n",
    "---\n",
    "\n",
    "## Learn More\n",
    "\n",
    "Build production-ready RAG systems with the **[Gen AI Crash Course](https://www.buildfastwithai.com/genai-course)** by Build Fast with AI!\n",
    "\n",
    "**Created by [Build Fast with AI](https://www.buildfastwithai.com)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
