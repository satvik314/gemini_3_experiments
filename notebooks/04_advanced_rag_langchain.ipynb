{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG with LangChain and Gemini 3 Pro\n",
    "\n",
    "> **Created by [Build Fast with AI](https://www.buildfastwithai.com)**\n",
    "\n",
    "This notebook demonstrates advanced RAG techniques using LangChain, including document loaders, text splitting, vector stores, and retrieval chains.\n",
    "\n",
    "## What you'll learn:\n",
    "- Using LangChain for RAG pipelines\n",
    "- Advanced text splitting strategies\n",
    "- Semantic search with reranking\n",
    "- Conversation memory with RAG\n",
    "- Building production-ready RAG systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain langchain-google-genai chromadb pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure API key\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "except:\n",
    "    GOOGLE_API_KEY = os.environ.get('GOOGLE_API_KEY', 'your-api-key-here')\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize LangChain Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Gemini model\n",
    "llm = GoogleGenerativeAI(\n",
    "    model=\"gemini-3-pro\",\n",
    "    temperature=0.7,\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "print(\"LangChain components initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Sample Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample documents about Python programming\n",
    "sample_text = \"\"\"\n",
    "Python Programming Guide\n",
    "\n",
    "Chapter 1: Introduction to Python\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python emphasizes code readability with\n",
    "its notable use of significant whitespace. It supports multiple programming paradigms including\n",
    "procedural, object-oriented, and functional programming.\n",
    "\n",
    "Chapter 2: Data Structures\n",
    "Python provides several built-in data structures. Lists are ordered, mutable collections that can\n",
    "contain items of different types. Tuples are similar to lists but are immutable. Dictionaries are\n",
    "key-value pairs that provide fast lookup times. Sets are unordered collections of unique elements.\n",
    "\n",
    "Chapter 3: Functions and Decorators\n",
    "Functions in Python are defined using the 'def' keyword. They can accept arguments and return values.\n",
    "Decorators are a powerful feature that allows you to modify or enhance functions without changing\n",
    "their source code. They are commonly used for logging, authentication, and memoization.\n",
    "\n",
    "Chapter 4: Object-Oriented Programming\n",
    "Python supports object-oriented programming with classes and inheritance. Classes are defined using\n",
    "the 'class' keyword. They can have attributes and methods. Inheritance allows you to create new\n",
    "classes based on existing ones, promoting code reuse.\n",
    "\n",
    "Chapter 5: File Handling\n",
    "Python makes it easy to work with files. The 'open()' function is used to open files in different\n",
    "modes (read, write, append). Context managers with the 'with' statement ensure proper resource\n",
    "management. Python can handle text files, binary files, and CSV files efficiently.\n",
    "\n",
    "Chapter 6: Error Handling\n",
    "Python uses try-except blocks for error handling. This allows you to catch and handle exceptions\n",
    "gracefully. Common exceptions include ValueError, TypeError, and FileNotFoundError. You can also\n",
    "create custom exceptions by inheriting from the Exception class.\n",
    "\n",
    "Chapter 7: Modules and Packages\n",
    "Python modules are files containing Python code. They can be imported using the 'import' statement.\n",
    "Packages are directories containing multiple modules. The Python Package Index (PyPI) hosts thousands\n",
    "of third-party packages that can be installed using pip.\n",
    "\n",
    "Chapter 8: Web Development\n",
    "Python is widely used for web development. Popular frameworks include Django, Flask, and FastAPI.\n",
    "Django is a full-featured framework that follows the MTV pattern. Flask is a lightweight framework\n",
    "that gives you more control. FastAPI is modern and fast, with automatic API documentation.\n",
    "\n",
    "Chapter 9: Data Science and Machine Learning\n",
    "Python is the leading language for data science and machine learning. NumPy provides numerical\n",
    "computing capabilities. Pandas offers data manipulation tools. Scikit-learn provides machine learning\n",
    "algorithms. TensorFlow and PyTorch are popular deep learning frameworks.\n",
    "\n",
    "Chapter 10: Best Practices\n",
    "Following PEP 8 style guide ensures code consistency. Write clear docstrings for functions and classes.\n",
    "Use virtual environments to manage dependencies. Write unit tests to ensure code quality. Use version\n",
    "control systems like Git. Comment your code when necessary but prefer self-documenting code.\n",
    "\"\"\"\n",
    "\n",
    "# Save to a temporary file\n",
    "with open('/tmp/python_guide.txt', 'w') as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "print(\"Sample document created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Text Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document\n",
    "loader = TextLoader('/tmp/python_guide.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Create text splitter with overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split documents\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Split document into {len(splits)} chunks\")\n",
    "print(f\"\\nFirst chunk preview:\\n{splits[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Chroma vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"python_guide\"\n",
    ")\n",
    "\n",
    "print(f\"Vector store created with {len(splits)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Basic Retrieval QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retrieval QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Test the chain\n",
    "question = \"What are Python decorators?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "display(Markdown(result['result']))\n",
    "\n",
    "print(\"\\n\\nSource documents:\")\n",
    "for i, doc in enumerate(result['source_documents'], 1):\n",
    "    print(f\"\\n{i}. {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Custom Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom prompt template\n",
    "custom_prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a helpful Python programming tutor. Use the following context to answer the question.\n",
    "If you don't know the answer, say so. Provide code examples when relevant.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Detailed Answer:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create QA chain with custom prompt\n",
    "qa_chain_custom = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": custom_prompt}\n",
    ")\n",
    "\n",
    "# Test with custom prompt\n",
    "question = \"How do I handle errors in Python?\"\n",
    "result = qa_chain_custom({\"query\": question})\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "display(Markdown(result['result']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conversational RAG with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key=\"answer\"\n",
    ")\n",
    "\n",
    "# Create conversational retrieval chain\n",
    "conv_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    memory=memory,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"Conversational RAG chain created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-turn conversation\n",
    "questions = [\n",
    "    \"What data structures does Python have?\",\n",
    "    \"Which one is mutable?\",\n",
    "    \"Can you give me an example of using it?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"User: {question}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    result = conv_chain({\"question\": question})\n",
    "    display(Markdown(result['answer']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Retrieval Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMR (Maximal Marginal Relevance) for diverse results\n",
    "mmr_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 4, \"fetch_k\": 10, \"lambda_mult\": 0.5}\n",
    ")\n",
    "\n",
    "# Test MMR retrieval\n",
    "query = \"Python frameworks\"\n",
    "docs = mmr_retriever.get_relevant_documents(query)\n",
    "\n",
    "print(f\"MMR retrieval for: '{query}'\\n\")\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"{i}. {doc.page_content[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Similarity Search with Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity search with relevance scores\n",
    "query = \"How to work with files?\"\n",
    "docs_with_scores = vectorstore.similarity_search_with_relevance_scores(\n",
    "    query,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Results with relevance scores:\\n\")\n",
    "\n",
    "for i, (doc, score) in enumerate(docs_with_scores, 1):\n",
    "    print(f\"{i}. Score: {score:.3f}\")\n",
    "    print(f\"   {doc.page_content[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Building a Production-Ready RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionRAGSystem:\n",
    "    def __init__(self, vectorstore, llm, embeddings):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "        self.embeddings = embeddings\n",
    "        self.conversation_history = []\n",
    "        \n",
    "    def ask(\n",
    "        self,\n",
    "        question,\n",
    "        k=3,\n",
    "        search_type=\"similarity\",\n",
    "        use_context=True,\n",
    "        show_sources=True\n",
    "    ):\n",
    "        \"\"\"Ask a question with advanced retrieval.\"\"\"\n",
    "        # Retrieve relevant documents\n",
    "        if search_type == \"mmr\":\n",
    "            retriever = self.vectorstore.as_retriever(\n",
    "                search_type=\"mmr\",\n",
    "                search_kwargs={\"k\": k, \"fetch_k\": k*2}\n",
    "            )\n",
    "        else:\n",
    "            retriever = self.vectorstore.as_retriever(\n",
    "                search_kwargs={\"k\": k}\n",
    "            )\n",
    "        \n",
    "        docs = retriever.get_relevant_documents(question)\n",
    "        \n",
    "        # Build context\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        # Add conversation history if requested\n",
    "        history_context = \"\"\n",
    "        if use_context and self.conversation_history:\n",
    "            history_context = \"\\n\".join([\n",
    "                f\"Q: {h['question']}\\nA: {h['answer']}\"\n",
    "                for h in self.conversation_history[-2:]\n",
    "            ])\n",
    "        \n",
    "        # Build prompt\n",
    "        prompt = f\"\"\"\n",
    "You are a helpful assistant. Answer the question based on the context provided.\n",
    "\n",
    "{f'Previous conversation:\\n{history_context}\\n\\n' if history_context else ''}\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "        \n",
    "        # Generate answer\n",
    "        answer = self.llm.invoke(prompt)\n",
    "        \n",
    "        # Store in history\n",
    "        self.conversation_history.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"sources\": docs\n",
    "        })\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        display(Markdown(answer))\n",
    "        \n",
    "        if show_sources:\n",
    "            print(\"\\n\\nSources:\")\n",
    "            for i, doc in enumerate(docs, 1):\n",
    "                print(f\"\\n{i}. {doc.page_content[:100]}...\")\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def reset_history(self):\n",
    "        \"\"\"Reset conversation history.\"\"\"\n",
    "        self.conversation_history = []\n",
    "        print(\"Conversation history reset.\")\n",
    "\n",
    "# Create production RAG system\n",
    "rag_system = ProductionRAGSystem(vectorstore, llm, embeddings)\n",
    "\n",
    "# Test it\n",
    "rag_system.ask(\"What are the best practices for Python programming?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with follow-up questions\n",
    "rag_system.ask(\"Can you elaborate on virtual environments?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Build agentic systems with LangGraph\n",
    "- Create multi-agent workflows with CrewAI\n",
    "- Deploy RAG applications with Streamlit\n",
    "\n",
    "---\n",
    "\n",
    "## Learn More\n",
    "\n",
    "Master advanced RAG techniques with the **[Gen AI Crash Course](https://www.buildfastwithai.com/genai-course)** by Build Fast with AI!\n",
    "\n",
    "**Created by [Build Fast with AI](https://www.buildfastwithai.com)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
