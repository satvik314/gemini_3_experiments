{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Long Context & Efficient Prompting with Gemini 3 Pro\n",
    "\n",
    "> **Created by [Build Fast with AI](https://www.buildfastwithai.com)**\n",
    "\n",
    "This notebook demonstrates how to work with long contexts and create efficient prompts using Gemini 3 Pro's extended context window.\n",
    "\n",
    "## What you'll learn:\n",
    "- Understanding context windows\n",
    "- Working with long documents\n",
    "- Prompt optimization techniques\n",
    "- Context management strategies\n",
    "- Token counting and optimization\n",
    "- Best practices for long-form content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-generativeai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown, display\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure API key\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "except:\n",
    "    GOOGLE_API_KEY = os.environ.get('GOOGLE_API_KEY', 'your-api-key-here')\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Understanding Context Windows\n",
    "\n",
    "Gemini 3 Pro has a large context window that allows processing extensive amounts of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model information\n",
    "for model in genai.list_models():\n",
    "    if 'gemini-3-pro' in model.name:\n",
    "        print(f\"Model: {model.name}\")\n",
    "        print(f\"Display Name: {model.display_name}\")\n",
    "        print(f\"Input Token Limit: {model.input_token_limit}\")\n",
    "        print(f\"Output Token Limit: {model.output_token_limit}\")\n",
    "        print(f\"Supported Methods: {model.supported_generation_methods}\")\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Token Counting\n",
    "\n",
    "Understanding how to count tokens is crucial for working with long contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_tokens(text: str) -> int:\n",
    "    \"\"\"Estimate token count (rough approximation).\"\"\"\n",
    "    # Rough estimate: ~4 characters per token\n",
    "    return len(text) // 4\n",
    "\n",
    "def count_tokens_gemini(text: str) -> int:\n",
    "    \"\"\"Count tokens using Gemini's token counter.\"\"\"\n",
    "    model = genai.GenerativeModel('gemini-3-pro')\n",
    "    return model.count_tokens(text).total_tokens\n",
    "\n",
    "# Test token counting\n",
    "sample_texts = [\n",
    "    \"Hello, world!\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial intelligence is transforming the way we interact with technology. \" * 10\n",
    "]\n",
    "\n",
    "print(\"Token Counting Comparison:\\n\")\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    estimated = estimate_tokens(text)\n",
    "    actual = count_tokens_gemini(text)\n",
    "    \n",
    "    print(f\"Text {i}:\")\n",
    "    print(f\"  Characters: {len(text)}\")\n",
    "    print(f\"  Words: {len(text.split())}\")\n",
    "    print(f\"  Estimated tokens: {estimated}\")\n",
    "    print(f\"  Actual tokens: {actual}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Processing Long Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a long document\n",
    "def generate_long_document():\n",
    "    \"\"\"Generate a sample long document.\"\"\"\n",
    "    sections = [\n",
    "        {\n",
    "            \"title\": \"Introduction to Machine Learning\",\n",
    "            \"content\": \"Machine learning is a subset of artificial intelligence that enables \" \n",
    "                      \"computers to learn from data without being explicitly programmed. \" * 20\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Supervised Learning\",\n",
    "            \"content\": \"Supervised learning involves training models on labeled data where the \" \n",
    "                      \"correct output is known. Common algorithms include linear regression, \" \n",
    "                      \"decision trees, and neural networks. \" * 20\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Unsupervised Learning\",\n",
    "            \"content\": \"Unsupervised learning works with unlabeled data to discover hidden patterns. \" \n",
    "                      \"Clustering algorithms like K-means and dimensionality reduction techniques \" \n",
    "                      \"like PCA are popular methods. \" * 20\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Deep Learning\",\n",
    "            \"content\": \"Deep learning uses neural networks with multiple layers to process complex \" \n",
    "                      \"patterns in large datasets. It has revolutionized fields like computer vision, \" \n",
    "                      \"natural language processing, and speech recognition. \" * 20\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Applications\",\n",
    "            \"content\": \"Machine learning is applied in numerous domains including healthcare, finance, \" \n",
    "                      \"autonomous vehicles, recommendation systems, and fraud detection. \" * 20\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    document = \"\\n\\n\".join([\n",
    "        f\"## {section['title']}\\n\\n{section['content']}\"\n",
    "        for section in sections\n",
    "    ])\n",
    "    \n",
    "    return document\n",
    "\n",
    "long_doc = generate_long_document()\n",
    "\n",
    "print(f\"Document Statistics:\")\n",
    "print(f\"  Characters: {len(long_doc):,}\")\n",
    "print(f\"  Words: {len(long_doc.split()):,}\")\n",
    "print(f\"  Estimated tokens: {estimate_tokens(long_doc):,}\")\n",
    "print(f\"  Actual tokens: {count_tokens_gemini(long_doc):,}\")\n",
    "print(f\"\\nFirst 500 characters:\")\n",
    "print(long_doc[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the long document\n",
    "model = genai.GenerativeModel('gemini-3-pro')\n",
    "\n",
    "# Various operations on long context\n",
    "operations = [\n",
    "    (\"Summarize\", \"Provide a concise summary of this document in 3-4 sentences.\"),\n",
    "    (\"Key Points\", \"Extract the 5 most important key points from this document.\"),\n",
    "    (\"Questions\", \"Generate 3 questions that this document answers.\"),\n",
    "    (\"Critique\", \"What topics are missing that should be included?\")\n",
    "]\n",
    "\n",
    "for operation_name, instruction in operations:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Operation: {operation_name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    prompt = f\"{instruction}\\n\\nDocument:\\n{long_doc}\"\n",
    "    response = model.generate_content(prompt)\n",
    "    \n",
    "    display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Efficient Prompting Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientPrompter:\n",
    "    \"\"\"Helper class for efficient prompt engineering.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = genai.GenerativeModel('gemini-3-pro')\n",
    "    \n",
    "    def structured_prompt(self, task: str, context: str, constraints: list = None,\n",
    "                         examples: list = None, output_format: str = None) -> str:\n",
    "        \"\"\"Create a well-structured prompt.\"\"\"\n",
    "        parts = []\n",
    "        \n",
    "        # Task\n",
    "        parts.append(f\"**Task:** {task}\\n\")\n",
    "        \n",
    "        # Context\n",
    "        if context:\n",
    "            parts.append(f\"**Context:**\\n{context}\\n\")\n",
    "        \n",
    "        # Constraints\n",
    "        if constraints:\n",
    "            parts.append(\"**Constraints:**\\n\" + \"\\n\".join(f\"- {c}\" for c in constraints) + \"\\n\")\n",
    "        \n",
    "        # Examples\n",
    "        if examples:\n",
    "            parts.append(\"**Examples:**\\n\" + \"\\n\".join(f\"{i}. {ex}\" for i, ex in enumerate(examples, 1)) + \"\\n\")\n",
    "        \n",
    "        # Output format\n",
    "        if output_format:\n",
    "            parts.append(f\"**Output Format:** {output_format}\\n\")\n",
    "        \n",
    "        return \"\\n\".join(parts)\n",
    "    \n",
    "    def chain_of_thought(self, question: str, context: str = \"\") -> str:\n",
    "        \"\"\"Use chain-of-thought prompting.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Question: {question}\n",
    "        \n",
    "        {f'Context: {context}' if context else ''}\n",
    "        \n",
    "        Let's approach this step-by-step:\n",
    "        1. First, let's understand what we're being asked\n",
    "        2. Then, let's identify the relevant information\n",
    "        3. Next, let's reason through the problem\n",
    "        4. Finally, let's arrive at our answer\n",
    "        \n",
    "        Please provide your reasoning for each step.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.model.generate_content(prompt)\n",
    "        return response.text\n",
    "    \n",
    "    def few_shot_prompt(self, task: str, examples: list, query: str) -> str:\n",
    "        \"\"\"Create a few-shot learning prompt.\"\"\"\n",
    "        prompt = f\"Task: {task}\\n\\nExamples:\\n\"\n",
    "        \n",
    "        for i, example in enumerate(examples, 1):\n",
    "            prompt += f\"\\nExample {i}:\\n\"\n",
    "            prompt += f\"Input: {example['input']}\\n\"\n",
    "            prompt += f\"Output: {example['output']}\\n\"\n",
    "        \n",
    "        prompt += f\"\\nNow, apply this to:\\nInput: {query}\\nOutput:\"\n",
    "        \n",
    "        response = self.model.generate_content(prompt)\n",
    "        return response.text\n",
    "\n",
    "# Test efficient prompting\n",
    "prompter = EfficientPrompter()\n",
    "\n",
    "# Example 1: Structured prompt\n",
    "print(\"Example 1: Structured Prompt\\n\")\n",
    "structured = prompter.structured_prompt(\n",
    "    task=\"Analyze the sentiment of customer reviews\",\n",
    "    context=\"Review: 'This product exceeded my expectations! Great quality and fast shipping.'\",\n",
    "    constraints=[\n",
    "        \"Classify as Positive, Negative, or Neutral\",\n",
    "        \"Provide confidence score (0-1)\",\n",
    "        \"Identify key phrases\"\n",
    "    ],\n",
    "    output_format=\"JSON with fields: sentiment, confidence, key_phrases\"\n",
    ")\n",
    "\n",
    "print(structured)\n",
    "response = prompter.model.generate_content(structured)\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Chain of Thought\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Example 2: Chain of Thought Prompting\\n\")\n",
    "\n",
    "cot_response = prompter.chain_of_thought(\n",
    "    question=\"If a store has a 20% discount on an item that costs $150, and then applies an additional 10% off the discounted price, what is the final price?\",\n",
    "    context=\"\"\n",
    ")\n",
    "\n",
    "display(Markdown(cot_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Few-Shot Learning\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Example 3: Few-Shot Learning\\n\")\n",
    "\n",
    "few_shot_response = prompter.few_shot_prompt(\n",
    "    task=\"Convert technical jargon into simple language\",\n",
    "    examples=[\n",
    "        {\n",
    "            \"input\": \"The API endpoint returns a JSON payload containing user metadata.\",\n",
    "            \"output\": \"The web service sends back user information in a structured format.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"We need to optimize the database queries to reduce latency.\",\n",
    "            \"output\": \"We need to make the database faster by improving how we ask for information.\"\n",
    "        }\n",
    "    ],\n",
    "    query=\"The machine learning model achieved 94% accuracy on the validation set.\"\n",
    ")\n",
    "\n",
    "display(Markdown(few_shot_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Context Management Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextManager:\n",
    "    \"\"\"Manage long contexts efficiently.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_tokens: int = 10000):\n",
    "        self.model = genai.GenerativeModel('gemini-3-pro')\n",
    "        self.max_tokens = max_tokens\n",
    "        self.context = []\n",
    "    \n",
    "    def add_to_context(self, text: str, role: str = \"user\"):\n",
    "        \"\"\"Add text to context with role.\"\"\"\n",
    "        self.context.append({\n",
    "            \"role\": role,\n",
    "            \"content\": text,\n",
    "            \"tokens\": count_tokens_gemini(text)\n",
    "        })\n",
    "        self._manage_context_size()\n",
    "    \n",
    "    def _manage_context_size(self):\n",
    "        \"\"\"Ensure context doesn't exceed max tokens.\"\"\"\n",
    "        total_tokens = sum(item[\"tokens\"] for item in self.context)\n",
    "        \n",
    "        while total_tokens > self.max_tokens and len(self.context) > 1:\n",
    "            # Remove oldest non-system messages\n",
    "            removed = self.context.pop(0)\n",
    "            total_tokens -= removed[\"tokens\"]\n",
    "            print(f\"Removed old context (saved {removed['tokens']} tokens)\")\n",
    "    \n",
    "    def get_context_string(self) -> str:\n",
    "        \"\"\"Get formatted context string.\"\"\"\n",
    "        return \"\\n\\n\".join([\n",
    "            f\"{item['role'].upper()}: {item['content']}\"\n",
    "            for item in self.context\n",
    "        ])\n",
    "    \n",
    "    def query_with_context(self, question: str) -> str:\n",
    "        \"\"\"Query using managed context.\"\"\"\n",
    "        self.add_to_context(question, \"user\")\n",
    "        \n",
    "        full_prompt = self.get_context_string()\n",
    "        response = self.model.generate_content(full_prompt)\n",
    "        \n",
    "        self.add_to_context(response.text, \"assistant\")\n",
    "        \n",
    "        return response.text\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"Get context statistics.\"\"\"\n",
    "        total_tokens = sum(item[\"tokens\"] for item in self.context)\n",
    "        return {\n",
    "            \"messages\": len(self.context),\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"remaining_tokens\": self.max_tokens - total_tokens,\n",
    "            \"utilization\": f\"{(total_tokens / self.max_tokens) * 100:.1f}%\"\n",
    "        }\n",
    "\n",
    "# Test context management\n",
    "manager = ContextManager(max_tokens=1000)\n",
    "\n",
    "# Add context\n",
    "manager.add_to_context(\"You are a helpful programming tutor.\", \"system\")\n",
    "\n",
    "# Have a conversation\n",
    "questions = [\n",
    "    \"What is Python?\",\n",
    "    \"How do I create a list?\",\n",
    "    \"What's the difference between a list and a tuple?\",\n",
    "    \"Can you show me an example?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\nUser: {q}\")\n",
    "    response = manager.query_with_context(q)\n",
    "    print(f\"Assistant: {response[:200]}...\" if len(response) > 200 else f\"Assistant: {response}\")\n",
    "    print(f\"\\nContext Stats: {manager.get_stats()}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 7. Document Chunking Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentChunker:\n",
    "    \"\"\"Intelligent document chunking.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_chunk_tokens: int = 2000):\n",
    "        self.max_chunk_tokens = max_chunk_tokens\n",
    "    \n",
    "    def chunk_by_tokens(self, text: str, overlap: int = 100) -> list:\n",
    "        \"\"\"Chunk text by token count with overlap.\"\"\"\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        \n",
    "        # Approximate: 1.3 words per token\n",
    "        words_per_chunk = int(self.max_chunk_tokens * 1.3)\n",
    "        overlap_words = int(overlap * 1.3)\n",
    "        \n",
    "        for i in range(0, len(words), words_per_chunk - overlap_words):\n",
    "            chunk = \" \".join(words[i:i + words_per_chunk])\n",
    "            chunks.append(chunk)\n",
    "            \n",
    "            if i + words_per_chunk >= len(words):\n",
    "                break\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def chunk_by_paragraphs(self, text: str) -> list:\n",
    "        \"\"\"Chunk by paragraphs, combining small ones.\"\"\"\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        current_tokens = 0\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            para_tokens = estimate_tokens(para)\n",
    "            \n",
    "            if current_tokens + para_tokens > self.max_chunk_tokens:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = para\n",
    "                current_tokens = para_tokens\n",
    "            else:\n",
    "                current_chunk += \"\\n\\n\" + para\n",
    "                current_tokens += para_tokens\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def chunk_by_sentences(self, text: str) -> list:\n",
    "        \"\"\"Chunk by sentences.\"\"\"\n",
    "        import re\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        current_tokens = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_tokens = estimate_tokens(sentence)\n",
    "            \n",
    "            if current_tokens + sentence_tokens > self.max_chunk_tokens:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence\n",
    "                current_tokens = sentence_tokens\n",
    "            else:\n",
    "                current_chunk += \" \" + sentence\n",
    "                current_tokens += sentence_tokens\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# Test chunking strategies\n",
    "chunker = DocumentChunker(max_chunk_tokens=500)\n",
    "\n",
    "strategies = {\n",
    "    \"Token-based\": chunker.chunk_by_tokens,\n",
    "    \"Paragraph-based\": chunker.chunk_by_paragraphs,\n",
    "    \"Sentence-based\": chunker.chunk_by_sentences\n",
    "}\n",
    "\n",
    "for strategy_name, strategy_func in strategies.items():\n",
    "    chunks = strategy_func(long_doc)\n",
    "    \n",
    "    print(f\"\\n{strategy_name} Chunking:\")\n",
    "    print(f\"  Total chunks: {len(chunks)}\")\n",
    "    print(f\"  Avg tokens per chunk: {sum(estimate_tokens(c) for c in chunks) / len(chunks):.0f}\")\n",
    "    print(f\"  First chunk preview: {chunks[0][:100]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 8. Summarization of Long Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongDocumentSummarizer:\n",
    "    \"\"\"Summarize documents that exceed context window.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = genai.GenerativeModel('gemini-3-pro')\n",
    "        self.chunker = DocumentChunker(max_chunk_tokens=3000)\n",
    "    \n",
    "    def summarize_chunks(self, text: str) -> str:\n",
    "        \"\"\"Summarize by processing chunks.\"\"\"\n",
    "        chunks = self.chunker.chunk_by_paragraphs(text)\n",
    "        \n",
    "        print(f\"Processing {len(chunks)} chunks...\\n\")\n",
    "        \n",
    "        chunk_summaries = []\n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            print(f\"Summarizing chunk {i}/{len(chunks)}...\")\n",
    "            \n",
    "            response = self.model.generate_content(\n",
    "                f\"Summarize this section concisely:\\n\\n{chunk}\"\n",
    "            )\n",
    "            chunk_summaries.append(response.text)\n",
    "        \n",
    "        # Combine summaries\n",
    "        combined = \"\\n\\n\".join(chunk_summaries)\n",
    "        \n",
    "        # Final summary\n",
    "        print(\"\\nCreating final summary...\")\n",
    "        final_response = self.model.generate_content(\n",
    "            f\"Create a comprehensive summary from these section summaries:\\n\\n{combined}\"\n",
    "        )\n",
    "        \n",
    "        return final_response.text\n",
    "    \n",
    "    def map_reduce_summarize(self, text: str) -> str:\n",
    "        \"\"\"Use map-reduce strategy for summarization.\"\"\"\n",
    "        chunks = self.chunker.chunk_by_paragraphs(text)\n",
    "        \n",
    "        # Map: Summarize each chunk\n",
    "        summaries = []\n",
    "        for chunk in chunks:\n",
    "            response = self.model.generate_content(\n",
    "                f\"Extract key points from this text:\\n\\n{chunk}\"\n",
    "            )\n",
    "            summaries.append(response.text)\n",
    "        \n",
    "        # Reduce: Combine summaries\n",
    "        combined = \"\\n\".join(summaries)\n",
    "        \n",
    "        final = self.model.generate_content(\n",
    "            f\"Synthesize these key points into a coherent summary:\\n\\n{combined}\"\n",
    "        )\n",
    "        \n",
    "        return final.text\n",
    "\n",
    "# Test summarization\n",
    "summarizer = LongDocumentSummarizer()\n",
    "\n",
    "print(\"Summarizing long document...\\n\")\n",
    "summary = summarizer.summarize_chunks(long_doc)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Final Summary:\")\n",
    "print(\"=\"*80)\n",
    "display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 9. Question Answering Over Long Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongDocumentQA:\n",
    "    \"\"\"Answer questions about long documents.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = genai.GenerativeModel('gemini-3-pro')\n",
    "        self.chunker = DocumentChunker(max_chunk_tokens=2000)\n",
    "    \n",
    "    def find_relevant_chunks(self, question: str, chunks: list, top_k: int = 3) -> list:\n",
    "        \"\"\"Find most relevant chunks for the question.\"\"\"\n",
    "        # Simple relevance scoring (in production, use embeddings)\n",
    "        question_words = set(question.lower().split())\n",
    "        \n",
    "        scored_chunks = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_words = set(chunk.lower().split())\n",
    "            overlap = len(question_words & chunk_words)\n",
    "            scored_chunks.append((i, chunk, overlap))\n",
    "        \n",
    "        # Sort by score and return top k\n",
    "        scored_chunks.sort(key=lambda x: x[2], reverse=True)\n",
    "        return [chunk for _, chunk, _ in scored_chunks[:top_k]]\n",
    "    \n",
    "    def answer_question(self, question: str, document: str) -> dict:\n",
    "        \"\"\"Answer a question about the document.\"\"\"\n",
    "        # Chunk the document\n",
    "        chunks = self.chunker.chunk_by_paragraphs(document)\n",
    "        \n",
    "        # Find relevant chunks\n",
    "        relevant_chunks = self.find_relevant_chunks(question, chunks, top_k=3)\n",
    "        \n",
    "        # Combine relevant context\n",
    "        context = \"\\n\\n\".join(relevant_chunks)\n",
    "        \n",
    "        # Generate answer\n",
    "        prompt = f\"\"\"\n",
    "        Answer the following question based on the provided context.\n",
    "        If the answer is not in the context, say so.\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.model.generate_content(prompt)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response.text,\n",
    "            \"chunks_used\": len(relevant_chunks),\n",
    "            \"context_length\": len(context)\n",
    "        }\n",
    "\n",
    "# Test Q&A\n",
    "qa_system = LongDocumentQA()\n",
    "\n",
    "questions = [\n",
    "    \"What is supervised learning?\",\n",
    "    \"What are some applications of machine learning?\",\n",
    "    \"How does deep learning differ from traditional machine learning?\",\n",
    "    \"What is quantum computing?\"  # Not in document\n",
    "]\n",
    "\n",
    "print(\"Question Answering Test:\\n\")\n",
    "for q in questions:\n",
    "    print(f\"Q: {q}\")\n",
    "    result = qa_system.answer_question(q, long_doc)\n",
    "    print(f\"A: {result['answer']}\")\n",
    "    print(f\"   (Used {result['chunks_used']} chunks, {result['context_length']} chars)\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 10. Best Practices for Long Context Processing\n",
    "\n",
    "### Context Window Management:\n",
    "\n",
    "1. **Know Your Limits**: Understand the model's context window\n",
    "2. **Token Counting**: Always count tokens before sending\n",
    "3. **Graceful Degradation**: Have fallback strategies for oversized inputs\n",
    "4. **Context Pruning**: Remove less relevant information when needed\n",
    "\n",
    "### Chunking Strategies:\n",
    "\n",
    "1. **Semantic Boundaries**: Chunk at natural boundaries (paragraphs, sections)\n",
    "2. **Overlap**: Use overlapping chunks to maintain context\n",
    "3. **Adaptive Sizing**: Adjust chunk size based on content structure\n",
    "4. **Metadata**: Preserve metadata about chunk position\n",
    "\n",
    "### Prompt Optimization:\n",
    "\n",
    "1. **Front-Load Instructions**: Put key instructions early\n",
    "2. **Clear Structure**: Use clear sections and formatting\n",
    "3. **Concise Context**: Include only relevant information\n",
    "4. **Example-Driven**: Use examples for complex tasks\n",
    "\n",
    "### Performance:\n",
    "\n",
    "1. **Parallel Processing**: Process independent chunks in parallel\n",
    "2. **Caching**: Cache results for repeated operations\n",
    "3. **Incremental Processing**: Process streams incrementally\n",
    "4. **Batch Operations**: Group similar operations\n",
    "\n",
    "### Quality:\n",
    "\n",
    "1. **Validation**: Validate outputs for consistency\n",
    "2. **Cross-Referencing**: Verify answers across chunks\n",
    "3. **Confidence Scoring**: Include confidence in responses\n",
    "4. **Human Review**: Enable human oversight for critical tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 11. Advanced Techniques\n",
    "\n",
    "### Map-Reduce Pattern:\n",
    "- Map: Process each chunk independently\n",
    "- Reduce: Combine results into final output\n",
    "- Use for: Summarization, extraction, analysis\n",
    "\n",
    "### Iterative Refinement:\n",
    "- First pass: Quick extraction\n",
    "- Second pass: Refinement and verification\n",
    "- Use for: Complex analysis, fact-checking\n",
    "\n",
    "### Hierarchical Processing:\n",
    "- Level 1: Process small sections\n",
    "- Level 2: Combine section results\n",
    "- Level 3: Final synthesis\n",
    "- Use for: Very long documents, books\n",
    "\n",
    "### Selective Context:\n",
    "- Identify key sections\n",
    "- Focus processing on relevant parts\n",
    "- Use for: Q&A, targeted extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 12. Real-World Applications\n",
    "\n",
    "### Long Context Use Cases:\n",
    "\n",
    "1. **Legal Document Analysis**:\n",
    "   - Contract review\n",
    "   - Compliance checking\n",
    "   - Case law research\n",
    "\n",
    "2. **Research Paper Processing**:\n",
    "   - Literature review\n",
    "   - Citation analysis\n",
    "   - Methodology extraction\n",
    "\n",
    "3. **Business Intelligence**:\n",
    "   - Report analysis\n",
    "   - Trend identification\n",
    "   - Competitive analysis\n",
    "\n",
    "4. **Content Creation**:\n",
    "   - Book summarization\n",
    "   - Long-form content generation\n",
    "   - Editorial review\n",
    "\n",
    "5. **Customer Support**:\n",
    "   - Knowledge base Q&A\n",
    "   - Ticket analysis\n",
    "   - Documentation search\n",
    "\n",
    "6. **Code Analysis**:\n",
    "   - Large codebase review\n",
    "   - Documentation generation\n",
    "   - Bug detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Continue exploring advanced AI capabilities:\n",
    "- Build production-grade RAG systems\n",
    "- Implement multi-agent workflows\n",
    "- Create domain-specific AI assistants\n",
    "- Deploy at scale with monitoring\n",
    "\n",
    "---\n",
    "\n",
    "## Learn More\n",
    "\n",
    "Master advanced AI techniques and build production systems with the **[Gen AI Crash Course](https://www.buildfastwithai.com/genai-course)** by Build Fast with AI!\n",
    "\n",
    "**Created by [Build Fast with AI](https://www.buildfastwithai.com)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
