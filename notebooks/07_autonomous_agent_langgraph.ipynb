{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Autonomous Agent with LangGraph for Data Analysis\n",
    "\n",
    "> **Created by [Build Fast with AI](https://www.buildfastwithai.com)**\n",
    "\n",
    "This notebook demonstrates how to build an autonomous data analysis agent using LangGraph and Gemini 3 Pro.\n",
    "\n",
    "## What you'll learn:\n",
    "- Building autonomous agents with LangGraph\n",
    "- Creating agents that analyze data\n",
    "- Implementing multi-step reasoning\n",
    "- Using tools for data manipulation\n",
    "- Agent state management\n",
    "- Visualization generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langgraph langchain langchain-google-genai pandas matplotlib seaborn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import TypedDict, Annotated, List, Dict, Any\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure API key\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "except:\n",
    "    GOOGLE_API_KEY = os.environ.get('GOOGLE_API_KEY', 'your-api-key-here')\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Create Sample Dataset\n",
    "\n",
    "Let's create a sample sales dataset for our agent to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample sales data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('2024-01-01', periods=365, freq='D')\n",
    "products = ['Product A', 'Product B', 'Product C', 'Product D']\n",
    "regions = ['North', 'South', 'East', 'West']\n",
    "\n",
    "data = []\n",
    "for date in dates:\n",
    "    for product in products:\n",
    "        for region in regions:\n",
    "            sales = np.random.randint(50, 500)\n",
    "            revenue = sales * np.random.uniform(10, 100)\n",
    "            data.append({\n",
    "                'date': date,\n",
    "                'product': product,\n",
    "                'region': region,\n",
    "                'sales': sales,\n",
    "                'revenue': revenue\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['month'] = df['date'].dt.month\n",
    "df['quarter'] = df['date'].dt.quarter\n",
    "\n",
    "print(f\"Dataset created: {len(df)} rows\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Define Data Analysis Tools\n",
    "\n",
    "Create tools that the agent can use to analyze data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_basic_stats(column: str) -> str:\n",
    "    \"\"\"Get basic statistics for a column.\n",
    "    \n",
    "    Args:\n",
    "        column: Name of the column to analyze (sales, revenue, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        String with statistical summary\n",
    "    \"\"\"\n",
    "    if column not in df.columns:\n",
    "        return f\"Error: Column '{column}' not found. Available columns: {', '.join(df.columns)}\"\n",
    "    \n",
    "    stats = df[column].describe() if df[column].dtype in ['int64', 'float64'] else df[column].value_counts()\n",
    "    return f\"Statistics for {column}:\\n{stats.to_string()}\"\n",
    "\n",
    "@tool\n",
    "def group_by_analysis(group_column: str, agg_column: str, agg_func: str = 'sum') -> str:\n",
    "    \"\"\"Perform group by analysis.\n",
    "    \n",
    "    Args:\n",
    "        group_column: Column to group by (product, region, month, quarter)\n",
    "        agg_column: Column to aggregate (sales, revenue)\n",
    "        agg_func: Aggregation function (sum, mean, max, min)\n",
    "    \n",
    "    Returns:\n",
    "        String with grouped results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = df.groupby(group_column)[agg_column].agg(agg_func).sort_values(ascending=False)\n",
    "        return f\"{agg_func.upper()} of {agg_column} by {group_column}:\\n{result.to_string()}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def get_top_performers(metric: str, top_n: int = 5, by: str = 'product') -> str:\n",
    "    \"\"\"Get top performers.\n",
    "    \n",
    "    Args:\n",
    "        metric: Metric to rank by (sales, revenue)\n",
    "        top_n: Number of top performers to return\n",
    "        by: Group by column (product, region)\n",
    "    \n",
    "    Returns:\n",
    "        String with top performers\n",
    "    \"\"\"\n",
    "    result = df.groupby(by)[metric].sum().sort_values(ascending=False).head(top_n)\n",
    "    return f\"Top {top_n} {by}s by {metric}:\\n{result.to_string()}\"\n",
    "\n",
    "@tool\n",
    "def calculate_growth(column: str, period: str = 'month') -> str:\n",
    "    \"\"\"Calculate growth over time.\n",
    "    \n",
    "    Args:\n",
    "        column: Column to analyze (sales, revenue)\n",
    "        period: Time period (month, quarter)\n",
    "    \n",
    "    Returns:\n",
    "        String with growth analysis\n",
    "    \"\"\"\n",
    "    time_series = df.groupby(period)[column].sum()\n",
    "    growth = time_series.pct_change() * 100\n",
    "    \n",
    "    result = pd.DataFrame({\n",
    "        'total': time_series,\n",
    "        'growth_%': growth\n",
    "    })\n",
    "    \n",
    "    return f\"Growth analysis for {column} by {period}:\\n{result.to_string()}\"\n",
    "\n",
    "@tool\n",
    "def find_trends(column: str, group_by: str = 'month') -> str:\n",
    "    \"\"\"Find trends in data.\n",
    "    \n",
    "    Args:\n",
    "        column: Column to analyze\n",
    "        group_by: Time period to group by\n",
    "    \n",
    "    Returns:\n",
    "        String with trend analysis\n",
    "    \"\"\"\n",
    "    time_series = df.groupby(group_by)[column].mean()\n",
    "    trend = \"increasing\" if time_series.iloc[-1] > time_series.iloc[0] else \"decreasing\"\n",
    "    change_pct = ((time_series.iloc[-1] - time_series.iloc[0]) / time_series.iloc[0]) * 100\n",
    "    \n",
    "    return f\"Trend for {column}: {trend} ({change_pct:.2f}% change)\\nDetails:\\n{time_series.to_string()}\"\n",
    "\n",
    "@tool\n",
    "def correlation_analysis(col1: str, col2: str) -> str:\n",
    "    \"\"\"Calculate correlation between two columns.\n",
    "    \n",
    "    Args:\n",
    "        col1: First column\n",
    "        col2: Second column\n",
    "    \n",
    "    Returns:\n",
    "        String with correlation coefficient\n",
    "    \"\"\"\n",
    "    if col1 not in df.columns or col2 not in df.columns:\n",
    "        return \"Error: One or both columns not found\"\n",
    "    \n",
    "    corr = df[col1].corr(df[col2])\n",
    "    return f\"Correlation between {col1} and {col2}: {corr:.4f}\"\n",
    "\n",
    "# List all tools\n",
    "tools = [\n",
    "    get_basic_stats,\n",
    "    group_by_analysis,\n",
    "    get_top_performers,\n",
    "    calculate_growth,\n",
    "    find_trends,\n",
    "    correlation_analysis\n",
    "]\n",
    "\n",
    "print(f\"Created {len(tools)} data analysis tools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Define Agent State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalysisState(TypedDict):\n",
    "    \"\"\"State for the data analysis agent.\"\"\"\n",
    "    query: str                          # User's analysis request\n",
    "    messages: List[Dict[str, str]]      # Conversation messages\n",
    "    analysis_steps: List[str]           # Steps taken by agent\n",
    "    findings: List[str]                 # Key findings\n",
    "    next_action: str                    # Next action to take\n",
    "    final_report: str                   # Final analysis report\n",
    "    iterations: int                     # Number of iterations\n",
    "\n",
    "print(\"Analysis state defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Initialize LLM with Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Gemini with tools\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-3-pro\",\n",
    "    temperature=0.1,  # Low temperature for consistent analysis\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "print(\"LLM initialized with data analysis tools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Define Agent Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITERATIONS = 10\n",
    "\n",
    "def planner_node(state: AnalysisState) -> AnalysisState:\n",
    "    \"\"\"Plan the analysis steps.\"\"\"\n",
    "    query = state['query']\n",
    "    \n",
    "    planning_prompt = f\"\"\"\n",
    "    You are a data analyst. Create an analysis plan for this request:\n",
    "    \n",
    "    Request: {query}\n",
    "    \n",
    "    Available data columns: date, product, region, sales, revenue, month, quarter\n",
    "    \n",
    "    Create a step-by-step plan using the available tools.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(planning_prompt)\n",
    "    \n",
    "    state['messages'].append({\n",
    "        \"role\": \"planner\",\n",
    "        \"content\": response.content\n",
    "    })\n",
    "    state['analysis_steps'].append(f\"Plan created: {response.content[:100]}...\")\n",
    "    state['next_action'] = 'analyze'\n",
    "    \n",
    "    return state\n",
    "\n",
    "def analyzer_node(state: AnalysisState) -> AnalysisState:\n",
    "    \"\"\"Perform analysis using tools.\"\"\"\n",
    "    # Check iteration limit\n",
    "    state['iterations'] = state.get('iterations', 0) + 1\n",
    "    \n",
    "    if state['iterations'] > MAX_ITERATIONS:\n",
    "        state['next_action'] = 'report'\n",
    "        return state\n",
    "    \n",
    "    # Build context from previous messages\n",
    "    context = \"\\n\".join([\n",
    "        f\"{msg['role']}: {msg['content']}\"\n",
    "        for msg in state['messages'][-5:]  # Last 5 messages\n",
    "    ])\n",
    "    \n",
    "    analysis_prompt = f\"\"\"\n",
    "    Context: {context}\n",
    "    \n",
    "    Continue the analysis. Use tools to gather data and insights.\n",
    "    When you have enough information, respond with \"ANALYSIS COMPLETE\" and summarize findings.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm_with_tools.invoke(analysis_prompt)\n",
    "    \n",
    "    state['messages'].append({\n",
    "        \"role\": \"analyzer\",\n",
    "        \"content\": response.content\n",
    "    })\n",
    "    \n",
    "    # Check if analysis is complete\n",
    "    if \"ANALYSIS COMPLETE\" in response.content.upper():\n",
    "        state['next_action'] = 'report'\n",
    "    elif hasattr(response, 'tool_calls') and response.tool_calls:\n",
    "        state['next_action'] = 'tools'\n",
    "    else:\n",
    "        state['next_action'] = 'analyze'\n",
    "    \n",
    "    return state\n",
    "\n",
    "def tool_executor_node(state: AnalysisState) -> AnalysisState:\n",
    "    \"\"\"Execute analysis tools.\"\"\"\n",
    "    last_message = state['messages'][-1]\n",
    "    \n",
    "    # Extract tool calls (simplified)\n",
    "    # In production, properly parse tool calls from LLM response\n",
    "    tool_results = []\n",
    "    \n",
    "    # Example: Execute get_basic_stats for sales\n",
    "    result = get_basic_stats.invoke({\"column\": \"sales\"})\n",
    "    tool_results.append(result)\n",
    "    \n",
    "    # Add results to state\n",
    "    results_text = \"\\n\\n\".join(tool_results)\n",
    "    state['messages'].append({\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"Tool Results:\\n{results_text}\"\n",
    "    })\n",
    "    state['findings'].append(results_text)\n",
    "    state['analysis_steps'].append(f\"Executed tools, got {len(tool_results)} results\")\n",
    "    state['next_action'] = 'analyze'\n",
    "    \n",
    "    return state\n",
    "\n",
    "def reporter_node(state: AnalysisState) -> AnalysisState:\n",
    "    \"\"\"Generate final report.\"\"\"\n",
    "    findings = \"\\n\".join(state['findings'])\n",
    "    \n",
    "    report_prompt = f\"\"\"\n",
    "    Create a comprehensive analysis report based on these findings:\n",
    "    \n",
    "    {findings}\n",
    "    \n",
    "    Original request: {state['query']}\n",
    "    \n",
    "    Format the report with:\n",
    "    1. Executive Summary\n",
    "    2. Key Findings\n",
    "    3. Detailed Analysis\n",
    "    4. Recommendations\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(report_prompt)\n",
    "    \n",
    "    state['final_report'] = response.content\n",
    "    state['next_action'] = 'end'\n",
    "    \n",
    "    return state\n",
    "\n",
    "def route_next(state: AnalysisState) -> str:\n",
    "    \"\"\"Determine next node.\"\"\"\n",
    "    action = state.get('next_action', 'end')\n",
    "    \n",
    "    if action == 'end':\n",
    "        return END\n",
    "    return action\n",
    "\n",
    "print(\"Agent nodes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 7. Build the Analysis Agent Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create workflow\n",
    "workflow = StateGraph(AnalysisState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"plan\", planner_node)\n",
    "workflow.add_node(\"analyze\", analyzer_node)\n",
    "workflow.add_node(\"tools\", tool_executor_node)\n",
    "workflow.add_node(\"report\", reporter_node)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"plan\")\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(\"plan\", \"analyze\")\n",
    "workflow.add_edge(\"tools\", \"analyze\")\n",
    "workflow.add_edge(\"report\", END)\n",
    "\n",
    "# Add conditional routing from analyze\n",
    "workflow.add_conditional_edges(\n",
    "    \"analyze\",\n",
    "    route_next,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        \"analyze\": \"analyze\",\n",
    "        \"report\": \"report\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Compile\n",
    "analysis_agent = workflow.compile()\n",
    "\n",
    "print(\"Analysis agent built successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 8. Run Analysis Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis(query: str):\n",
    "    \"\"\"Run the analysis agent.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Analysis Request: {query}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Initialize state\n",
    "    initial_state = {\n",
    "        \"query\": query,\n",
    "        \"messages\": [],\n",
    "        \"analysis_steps\": [],\n",
    "        \"findings\": [],\n",
    "        \"next_action\": \"plan\",\n",
    "        \"final_report\": \"\",\n",
    "        \"iterations\": 0\n",
    "    }\n",
    "    \n",
    "    # Run agent\n",
    "    result = analysis_agent.invoke(initial_state)\n",
    "    \n",
    "    # Display report\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(\"=\"*80)\n",
    "    display(Markdown(result['final_report']))\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example 1: Sales analysis\n",
    "result1 = run_analysis(\n",
    "    \"Analyze total sales and revenue by product. Which product performs best?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Regional analysis\n",
    "result2 = run_analysis(\n",
    "    \"Compare regional performance. Which region has the highest growth?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Trend analysis\n",
    "result3 = run_analysis(\n",
    "    \"Identify trends in monthly sales and revenue. Are we growing or declining?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 9. Manual Data Analysis with Tools\n",
    "\n",
    "You can also use the tools directly for quick analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic stats\n",
    "print(get_basic_stats.invoke({\"column\": \"revenue\"}))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Top products by sales\n",
    "print(get_top_performers.invoke({\"metric\": \"sales\", \"top_n\": 3, \"by\": \"product\"}))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Growth analysis\n",
    "print(calculate_growth.invoke({\"column\": \"revenue\", \"period\": \"quarter\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 10. Visualization Generation\n",
    "\n",
    "Create visualizations to complement the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales by product\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Total sales by product\n",
    "product_sales = df.groupby('product')['sales'].sum().sort_values(ascending=False)\n",
    "product_sales.plot(kind='bar', ax=axes[0, 0], color='skyblue')\n",
    "axes[0, 0].set_title('Total Sales by Product', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Sales')\n",
    "\n",
    "# Plot 2: Revenue by region\n",
    "region_revenue = df.groupby('region')['revenue'].sum().sort_values(ascending=False)\n",
    "region_revenue.plot(kind='bar', ax=axes[0, 1], color='lightcoral')\n",
    "axes[0, 1].set_title('Total Revenue by Region', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Revenue')\n",
    "\n",
    "# Plot 3: Monthly sales trend\n",
    "monthly_sales = df.groupby('month')['sales'].mean()\n",
    "monthly_sales.plot(kind='line', ax=axes[1, 0], marker='o', color='green')\n",
    "axes[1, 0].set_title('Average Monthly Sales Trend', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Month')\n",
    "axes[1, 0].set_ylabel('Average Sales')\n",
    "\n",
    "# Plot 4: Quarterly revenue by product\n",
    "quarterly_product = df.groupby(['quarter', 'product'])['revenue'].sum().unstack()\n",
    "quarterly_product.plot(kind='bar', ax=axes[1, 1], stacked=False)\n",
    "axes[1, 1].set_title('Quarterly Revenue by Product', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Quarter')\n",
    "axes[1, 1].set_ylabel('Revenue')\n",
    "axes[1, 1].legend(title='Product')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 11. Advanced Agent with Self-Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfCorrectingAnalysisAgent:\n",
    "    \"\"\"An agent that can validate and correct its own analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-3-pro\",\n",
    "            temperature=0.1,\n",
    "            google_api_key=GOOGLE_API_KEY\n",
    "        )\n",
    "        self.analysis_history = []\n",
    "    \n",
    "    def analyze(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Perform analysis with self-correction.\"\"\"\n",
    "        # Step 1: Initial analysis\n",
    "        analysis = self._perform_analysis(query)\n",
    "        \n",
    "        # Step 2: Validate\n",
    "        validation = self._validate_analysis(query, analysis)\n",
    "        \n",
    "        # Step 3: Correct if needed\n",
    "        if validation['needs_correction']:\n",
    "            analysis = self._correct_analysis(query, analysis, validation['issues'])\n",
    "        \n",
    "        # Save to history\n",
    "        self.analysis_history.append({\n",
    "            'query': query,\n",
    "            'analysis': analysis,\n",
    "            'validated': not validation['needs_correction']\n",
    "        })\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _perform_analysis(self, query: str) -> str:\n",
    "        \"\"\"Perform initial analysis.\"\"\"\n",
    "        prompt = f\"Analyze this data request: {query}\\nProvide detailed insights.\"\n",
    "        response = self.llm.invoke(prompt)\n",
    "        return response.content\n",
    "    \n",
    "    def _validate_analysis(self, query: str, analysis: str) -> Dict[str, Any]:\n",
    "        \"\"\"Validate the analysis.\"\"\"\n",
    "        validation_prompt = f\"\"\"\n",
    "        Original query: {query}\n",
    "        Analysis: {analysis}\n",
    "        \n",
    "        Validate this analysis. Check for:\n",
    "        1. Accuracy\n",
    "        2. Completeness\n",
    "        3. Logical consistency\n",
    "        \n",
    "        Respond with JSON: {{\"needs_correction\": true/false, \"issues\": [list of issues]}}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Simplified validation\n",
    "        return {\"needs_correction\": False, \"issues\": []}\n",
    "    \n",
    "    def _correct_analysis(self, query: str, analysis: str, issues: List[str]) -> str:\n",
    "        \"\"\"Correct the analysis.\"\"\"\n",
    "        correction_prompt = f\"\"\"\n",
    "        Original query: {query}\n",
    "        Previous analysis: {analysis}\n",
    "        Issues found: {', '.join(issues)}\n",
    "        \n",
    "        Provide a corrected analysis addressing these issues.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(correction_prompt)\n",
    "        return response.content\n",
    "\n",
    "# Test the self-correcting agent\n",
    "agent = SelfCorrectingAnalysisAgent()\n",
    "result = agent.analyze(\"What are the key insights from our sales data?\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## 12. Best Practices for Autonomous Agents\n",
    "\n",
    "### Key Principles:\n",
    "\n",
    "1. **Clear State Management**: Define explicit states and transitions\n",
    "2. **Tool Design**: Create focused, single-purpose tools\n",
    "3. **Iteration Limits**: Prevent infinite loops with max iterations\n",
    "4. **Error Handling**: Handle errors gracefully at each step\n",
    "5. **Validation**: Validate outputs before moving to next step\n",
    "6. **Logging**: Track agent decisions for debugging\n",
    "7. **Human-in-the-Loop**: Allow human review for critical decisions\n",
    "8. **Testing**: Test each node independently\n",
    "\n",
    "### Common Patterns:\n",
    "\n",
    "- **Plan-Execute-Reflect**: Plan → Execute → Validate → Report\n",
    "- **Multi-step Reasoning**: Break complex tasks into smaller steps\n",
    "- **Tool Composition**: Combine multiple tools for complex analysis\n",
    "- **Self-Correction**: Validate and correct outputs automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Explore more advanced topics:\n",
    "- Build Streamlit applications with Gemini\n",
    "- Implement multimodal analysis (text, images, video)\n",
    "- Create production-ready agent systems\n",
    "- Deploy agents at scale\n",
    "\n",
    "---\n",
    "\n",
    "## Learn More\n",
    "\n",
    "Master autonomous AI agents with the **[Gen AI Crash Course](https://www.buildfastwithai.com/genai-course)** by Build Fast with AI!\n",
    "\n",
    "**Created by [Build Fast with AI](https://www.buildfastwithai.com)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
